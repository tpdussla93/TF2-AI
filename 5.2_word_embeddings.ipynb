{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 워드 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "### 텍스트를 숫자로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "이 튜토리얼에는 단어 임베딩에 대한 소개가 포함되어 있습니다. 감정 분류 작업을 위해 간단한 Keras 모델을 사용하여 자신 만의 단어 임베딩을 훈련 한 다음 [Embedding Projector](http://projector.tensorflow.org) (아래 이미지 참조)에서 시각화합니다.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\" alt=\"Screenshot of the embedding projector\" width=\"400\"/>\n",
    "\n",
    "\n",
    "기계 학습 모델은 벡터 (숫자 배열)를 입력으로 사용합니다. 텍스트로 작업 할 때 가장 먼저해야 할 일은 문자열을 모델에 공급하기 전에 문자열을 숫자로 변환 (또는 텍스트를 \"벡터화\")하는 전략을 마련하는 것입니다. 이 섹션에서는 이를 위한 세 가지 전략을 살펴 봅니다.\n",
    "\n",
    "### 원-핫 인코딩\n",
    "\n",
    "첫 번째 아이디어로 어휘의 각 단어를 \"원-핫\"인코딩 할 수 있습니다. \"The cat sat on the mat\"라는 문장을 생각해보십시오. 이 문장의 어휘 (또는 고유 한 단어)는 (cat, mat, on, sat, the)입니다. 각 단어를 나타 내기 위해 어휘와 길이가 같은 0 벡터를 만든 다음 해당 단어에 해당하는 색인에 1을 배치합니다. 이 접근 방식은 다음 다이어그램에 나와 있습니다.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
    "\n",
    "문장의 인코딩을 포함하는 벡터를 만들기 위해 각 단어에 대해 원-핫 벡터를 연결할 수 있습니다.\n",
    "\n",
    "요점 : 이 접근 방식은 비효율적입니다. 원-핫 인코딩 된 벡터는 희소합니다 (즉, 대부분의 인덱스가 0 임). 어휘에 10,000 개의 단어가 있다고 상상해보십시오. 각 단어를 원-핫 인코딩하려면 요소의 99.99 %가 0 인 벡터를 만듭니다.\n",
    "\n",
    "### 각 단어를 고유 한 번호로 인코딩\n",
    "\n",
    "두 번째 방법은 고유 한 숫자를 사용하여 각 단어를 인코딩하는 것입니다. 위의 예를 계속하여 \"cat\"에 1을 할당하고 \"mat\"에 2를 할당 할 수 있습니다. 그런 다음 \"The cat sat on the mat\"라는 문장을 [5, 1, 4, 3, 5, 2]와 같은 고밀도 벡터로 인코딩 할 수 있습니다. 이 접근 방식은 효율적입니다. 희소 벡터 대신 이제 모든 요소가 가득 찬 조밀 한 벡터가 있습니다.\n",
    "\n",
    "그러나이 방법에는 두 가지 단점이 있습니다.\n",
    "\n",
    "* 정수 인코딩은 임의적입니다 (단어 간의 관계를 캡처하지 않음).\n",
    "\n",
    "* 정수 인코딩은 모델이 해석하기 어려울 수 있습니다. 예를 들어 선형 분류기는 각 특성에 대한 단일 가중치를 학습합니다. 두 단어의 유사성과 해당 인코딩의 유사성 간에는 관계가 없기 때문에이 기능 가중치 조합은 의미가 없습니다.\n",
    "\n",
    "### 단어 임베딩\n",
    "\n",
    "단어 임베딩은 유사한 단어가 유사한 인코딩을 갖는 효율적이고 조밀 한 표현을 사용하는 방법을 제공합니다. 중요한 것은이 인코딩을 직접 지정할 필요가 없다는 것입니다. 임베딩은 부동 소수점 값으로 구성된 조밀 한 벡터입니다 (벡터의 길이는 사용자가 지정하는 매개 변수 임). 임베딩 값을 수동으로 지정하는 대신 학습 가능한 매개 변수입니다 (모델이 조밀 한 레이어에 대한 가중치를 학습하는 것과 같은 방식으로 학습 중에 모델에서 학습 한 가중치). 8 차원 (작은 데이터 세트의 경우), 대규모 데이터 세트로 작업 할 때 최대 1024 차원의 단어 임베딩을 보는 것이 일반적입니다. 더 높은 차원의 임베딩은 단어 간의 세밀한 관계를 캡처 할 수 있지만 학습하는 데 더 많은 데이터가 필요합니다.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "\n",
    "위는 단어 임베딩에 대한 다이어그램입니다. 각 단어는 부동 소수점 값의 4 차원 벡터로 표시됩니다. 임베딩을 생각하는 또 다른 방법은 \"조회 테이블\"입니다. 이러한 가중치를 학습 한 후 표에서 해당하는 조밀 한 벡터를 찾아 각 단어를 인코딩 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:00.027127Z",
     "iopub.status.busy": "2021-01-13T04:09:00.026402Z",
     "iopub.status.idle": "2021-01-13T04:09:06.603000Z",
     "shell.execute_reply": "2021-01-13T04:09:06.602369Z"
    },
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "### IMDB 데이터 세트 다운로드\n",
    "\n",
    "튜토리얼을 통해 Large Movie Review Dataset 을 사용하게됩니다. 이 데이터 세트에서 감정 분류기 모델을 훈련시키고 그 과정에서 임베딩을 처음부터 배우게됩니다. 데이터 세트를 처음부터로드하는 방법에 대한 자세한 내용은 텍스트로드 가이드를 참조 하세요 .\n",
    "\n",
    "Keras 파일 유틸리티를 사용하여 데이터 세트를 다운로드하고 디렉토리를 살펴보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:06.609910Z",
     "iopub.status.busy": "2021-01-13T04:09:06.609205Z",
     "iopub.status.idle": "2021-01-13T04:09:27.313818Z",
     "shell.execute_reply": "2021-01-13T04:09:27.314295Z"
    },
    "id": "aPO4_UmfF0KH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                    extract=True, cache_dir='.',\n",
    "                                    cache_subdir='.keras/datasets')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": [
    "train/ 디렉토리를 살펴보십시오. 그것은 각각 긍정적 및 부정으로 표시된 영화 리뷰가있는 pos 및 neg 폴더를 가지고 있습니다. 이진 분류 모델을 학습하기 위해 pos 및 neg 폴더의 리뷰를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:27.319864Z",
     "iopub.status.busy": "2021-01-13T04:09:27.319178Z",
     "iopub.status.idle": "2021-01-13T04:09:27.321853Z",
     "shell.execute_reply": "2021-01-13T04:09:27.322302Z"
    },
    "id": "9-iOHJGN6SDu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O59BdioK8jY"
   },
   "source": [
    "train 디렉터리에는 교육 데이터 세트를 만들기 전에 제거해야하는 추가 폴더도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:27.326807Z",
     "iopub.status.busy": "2021-01-13T04:09:27.326131Z",
     "iopub.status.idle": "2021-01-13T04:09:28.221686Z",
     "shell.execute_reply": "2021-01-13T04:09:28.221008Z"
    },
    "id": "1_Vfi9oWMSh-"
   },
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": [
    "다음으로tf.data.Dataset 사용하여 tf.keras.preprocessing.text_dataset_from_directory . 이 텍스트 분류 자습서 에서이 유틸리티 사용에 대한 자세한 내용을 읽을 수 있습니다.\n",
    "\n",
    "train 디렉터리를 사용하여 유효성 검사를 위해 20 %로 분할 된 기차 및 유효성 검사 데이터 세트를 모두 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:28.227987Z",
     "iopub.status.busy": "2021-01-13T04:09:28.227113Z",
     "iopub.status.idle": "2021-01-13T04:09:31.882831Z",
     "shell.execute_reply": "2021-01-13T04:09:31.882253Z"
    },
    "id": "ItYD3TLkCOP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size, validation_split=0.2, \n",
    "    subset='training', seed=seed)\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir, batch_size=batch_size, validation_split=0.2, \n",
    "    subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": [
    "훈련 데이터 세트에서 몇 가지 영화 리뷰와 라벨 (1: positive, 0: negative) 을 살펴보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:31.888110Z",
     "iopub.status.busy": "2021-01-13T04:09:31.887383Z",
     "iopub.status.idle": "2021-01-13T04:09:32.278989Z",
     "shell.execute_reply": "2021-01-13T04:09:32.278358Z"
    },
    "id": "aTCbSkvkYmTT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b\"The original animated Dark Knight returns in this ace adventure movie that rivals Mask of Phantasm in its coolness. There's a lot of style and intelligence in Mystery of the Batwoman, so much more than Batman Forever or Batman and Robin.<br /><br />There's a new crime-fighter on the streets of Gotham. She dresses like a bat but she's not a grown-up Batgirl. And Batman is denying any affiliation with her. Meanwhile Bruce Wayne has to deal with the usual romances and detective work. But the Penguin, Bain and the local Mob makes things little more complicated.<br /><br />I didn't have high hopes for this 'un since being strongly let down but the weak Batman: Sub Zero (Robin isn't featured so much here!)but I was delighted with the imaginative and exciting set pieces, the clever plot and a cheeky sense of humor. This is definitely a movie no fan of Batman should be without. Keep your ears open for a really catchy song called 'Betcha Neva' which is featured prominently through-out.<br /><br />It's a shame the DVD isn't so great. Don't get me wrong there are some great features (the short 'Chase Me' is awesome) and a very cool Dolby 5.1 soundtrack but... the movie is presented in Pan and Scan. Batman: Mystery of the Batwoman was drawn and shot in 1.85:1 but this DVD is presented in 1.33:1 an in comparison to the widescreen clips shown on the features there IS picture cut off on both sides. I find this extremely annoying considering Mask of Phantasm was presented in anamorphic widescreen. Warner have had to re-release literally dozens of movies on DVD because people have complained about the lack of Original Aspect Ratio available on some titles. Why they chose to make that same mistake here again is beyond me.<br /><br />I would give this DVD 5/5 but the lack of OAR brings the overall score down to 4/5. It's a shame because widescreen would have completed a great DVD package.\"\n",
      "1 b'Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. It just never gets old, despite my having seen it some 15 or more times in the last 25 years. Paul Lukas\\' performance brings tears to my eyes, and Bette Davis, in one of her very few truly sympathetic roles, is a delight. The kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. And the mother\\'s slow awakening to what\\'s happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they\\'d all be \"up\" for this movie.'\n",
      "1 b'I claim no matter how hard I seek I\\'ll never find a better movie version of \"Othello\". If you love Kenneth Branagh\\'s magnificent masterpieces \"Much ado about nothing\" (1993) and \"Hamlet\" (1996) as much as I do I\\'m dead certain you\\'ll also find Oliver Parker\\'s \"Othello\" irresistible. Laurence Fishburne has been in a various splendid roles during his career. He was quite terrific in \"Boys n the hood\" (1991) - I\\'ve always considered his amusing role of Furious Styles as his very greatest achievement. That was, of course, way before I saw this.<br /><br />He plays the part of Othello and he is probably in the most challenging role of his whole career but he does a brilliant, fantastic job. Ir\\xc3\\xa8ne Jacob is absolutely charming Desdemona and Kenneth Branagh is just simply phenomenal in a most fascinating role of the story\\'s crooked, manipulate villain Iago. Marvelous \"Othello\" is part of the absolute elite among Shakespeare\\'s ingenious works. It deals with his favorite topics: crookedness, envy, deceitfulness and jealousy. This movie adaptation is certainly one of the finest films I\\'ve seen that\\'s based on William Shakespeare\\'s plays.'\n",
      "1 b\"Some people say this is the best film that PRC ever released, I'm not too sure about that since I have a fond place in my heart for some of their mysteries. I will say that this is probably one of the most unique films they, or any other studio, major or minor, ever released.<br /><br />The plot is simple. The ghost of a wrongly executed ferryman has returned to the swamp to kill all those who lynched him as well as all of their off spring. Into this mix comes the granddaughter of one ghosts victims, the current ferryman. She takes over the ferry business as the ghost closes in on the man she loves.<br /><br />Shrouded in dense fog and set primarily on the single swamp set this is more musical poem than regular feature film.Listen to the rhythms of the dialog, especially in the early scenes, their is poetical cadence to them. Likewise there is a similar cadence to the camera work as it travels back and forth across the swamp as if crossing back and forth across the door way between life and death, innocence and guilt. The film reminds me of an opera or oratorio or musical object lesson more than a normal horror film. Its an amazing piece of film making that is probably unique in film history.<br /><br />This isn't to guild the Lilly. This is a low budget horror/mystery that tells you a neat little story that will keep you entertained. Its tale of love and revenge is what matters here, not the poetical film making and it holds you attention first and foremost (the technical aspects just being window dressing.) If there is any real flaw its the cheapness of the production. The fog does create a mood but it also hides the fact that this swamp is entirely on dry land. The constant back and forth across it is okay for a while but even after 58 minutes you do wish that we could see something else.<br /><br />Don't get me wrong I do like the film a great deal. Its a good little film that I some how wish was slightly less poverty stricken. Its definitely worth a look if you can come across it.\"\n",
      "1 b'A journey of discovery, this film follows the lives of one family living in a sleepy, island town in British Columbia. Languorous and dreamy, the inhabitants are satisfied to allow life to go on around them until a young, fresh-faced teacher, with new ideas arrives and brings with her life from the mainland. Slowly, their indolent state is awakened, the father (and principal of the local school) looks for excitement, the mother for stability, the oldest daughter for love, and the youngest for power. While not an incredible or ground-breaking piece of cinema, the movie is quietly enjoyable and good for a tired night when the wind is blowing. Unfortunately, I doubt anyone outside of Canada will find it easily accessible.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(5):\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHV2pchDhzDn"
   },
   "source": [
    "### 성능을위한 데이터 세트 구성\n",
    "\n",
    "이는 I / O가 차단되지 않도록 데이터를로드 할 때 사용해야하는 두 가지 중요한 방법입니다.\n",
    "\n",
    ".cache() 데이터가 디스크에서로드 된 후 메모리에 데이터를 보관합니다. 이렇게하면 모델을 학습하는 동안 데이터 세트가 병목 현상이 발생하지 않습니다. 데이터 세트가 너무 커서 메모리에 맞지 않는 경우이 방법을 사용하여 성능이 뛰어난 온 디스크 캐시를 생성 할 수도 있습니다. 이는 많은 작은 파일보다 읽기가 더 효율적입니다.\n",
    "\n",
    ".prefetch() 학습 중에 데이터 전처리 및 모델 실행과 겹칩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:32.284161Z",
     "iopub.status.busy": "2021-01-13T04:09:32.283401Z",
     "iopub.status.idle": "2021-01-13T04:09:32.286803Z",
     "shell.execute_reply": "2021-01-13T04:09:32.287252Z"
    },
    "id": "Oz6k1IW7h1TO"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "### Embedding 레이어 사용\n",
    "\n",
    "Keras를 사용하면 단어 임베딩을 쉽게 사용할 수 있습니다. Embedding 레이어를 살펴보십시오.\n",
    "\n",
    "임베딩 레이어는 정수 인덱스 (특정 단어를 의미)에서 고밀도 벡터 (임베딩)로 매핑하는 조회 테이블로 이해할 수 있습니다. 임베딩의 차원 (또는 너비)은 Dense 레이어의 뉴런 수를 실험하는 것과 같은 방식으로 문제에 잘 맞는지 확인하기 위해 실험 할 수있는 매개 변수입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:32.291492Z",
     "iopub.status.busy": "2021-01-13T04:09:32.290814Z",
     "iopub.status.idle": "2021-01-13T04:09:32.301978Z",
     "shell.execute_reply": "2021-01-13T04:09:32.301448Z"
    },
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": [
    "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dKKV1L2Rk7e"
   },
   "source": [
    "Embedding 레이어를 만들면 다른 레이어와 마찬가지로 임베딩에 대한 가중치가 임의로 초기화됩니다. 훈련 중에 역 전파를 통해 점진적으로 조정됩니다. 학습 된 단어 임베딩은 모델이 학습 된 특정 문제에 대해 학습 된 것처럼 단어 간의 유사성을 대략적으로 인코딩합니다.\n",
    "\n",
    "임베딩 레이어에 정수를 전달하면 결과는 각 정수를 임베딩 테이블의 벡터로 대체합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:32.307339Z",
     "iopub.status.busy": "2021-01-13T04:09:32.306648Z",
     "iopub.status.idle": "2021-01-13T04:09:32.315475Z",
     "shell.execute_reply": "2021-01-13T04:09:32.314950Z"
    },
    "id": "0YUjPgP7w0PO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01470711,  0.04289306, -0.03442582, -0.00809392,  0.00425293],\n",
       "       [-0.0163384 ,  0.02025395,  0.01401388,  0.0180258 ,  0.03500709],\n",
       "       [ 0.02370551, -0.01547126,  0.0029506 ,  0.04565367,  0.03095836]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1,2,3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4PC4QzsxTGx"
   },
   "source": [
    "텍스트 또는 시퀀스 문제의 경우 Embedding 레이어는 (samples, sequence_length) 모양의 정수 2D 텐서를 사용하며, 여기서 각 항목은 정수 시퀀스입니다. 가변 길이의 시퀀스를 포함 할 수 있습니다. 모양 (32, 10) (길이 10의 32 시퀀스 배치) 또는 (64, 15) (길이 15의 64 시퀀스 배치 (64, 15) 배치 위의 임베딩 레이어로 공급할 수 있습니다.\n",
    "\n",
    "반환 된 텐서는 입력보다 축이 하나 더 많으며 임베딩 벡터는 새 마지막 축을 따라 정렬됩니다. (2, 3) 입력 배치를 전달하면 출력은 (2, 3, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:32.320306Z",
     "iopub.status.busy": "2021-01-13T04:09:32.319594Z",
     "iopub.status.idle": "2021-01-13T04:09:32.322874Z",
     "shell.execute_reply": "2021-01-13T04:09:32.323319Z"
    },
    "id": "vwSYepRjyRGy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result[0,1,:] = <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([-0.01470711,  0.04289306, -0.03442582, -0.00809392,  0.00425293],\n",
      "      dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "print(f'{result[0,1,:] = }')\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "시퀀스 배치가 입력으로 주어지면 임베딩 레이어는 (samples, sequence_length, embedding_dimensionality) 모양의 3D 부동 소수점 텐서를 반환합니다. 이 가변 길이 시퀀스에서 고정 표현으로 변환하려면 다양한 표준 접근 방식이 있습니다. Dense 레이어로 전달하기 전에 RNN, Attention 또는 풀링 레이어를 사용할 수 있습니다. 이 튜토리얼은 가장 간단하기 때문에 풀링을 사용합니다. RNN을 사용한 텍스트 분류 튜토리얼은 좋은 다음 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": [
    "다음으로 감정 분류 모델에 필요한 데이터 세트 전처리 단계를 정의합니다. 원하는 매개 변수로 TextVectorization 레이어를 초기화하여 영화 리뷰를 벡터화합니다. 텍스트 분류 튜토리얼에서 이 레이어 사용에 대해 자세히 알아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:32.345692Z",
     "iopub.status.busy": "2021-01-13T04:09:32.342229Z",
     "iopub.status.idle": "2021-01-13T04:09:35.492916Z",
     "shell.execute_reply": "2021-01-13T04:09:35.493403Z"
    },
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "### 분류 모델 만들기\n",
    "\n",
    "Keras Sequential API 를 사용하여 감정 분류 모델을 정의합니다. 이 경우 \"연속 단어 모음\"스타일 모델입니다.\n",
    "\n",
    "- TextVectorization 레이어는 문자열을 어휘 색인으로 변환합니다. 이미 vectorize_layer 를 TextVectorization 레이어로 초기화했으며 `text_ds` 로 `TextVectorization.adapt` 를 호출하여 어휘를 구축했습니다. 이제 vectorize_layer를 종단 간 분류 모델의 첫 번째 계층으로 사용하여 변형 된 문자열을 Embedding 계층에 공급할 수 있습니다.\n",
    "\n",
    "- Embedding 레이어는 정수로 인코딩 된 어휘를 가져와 각 단어 인덱스에 대한 임베딩 벡터를 찾습니다. 이러한 벡터는 모델 학습으로 학습됩니다. 벡터는 출력 배열에 차원을 추가합니다. 결과 차원은 다음과 같습니다. (batch, sequence, embedding) .\n",
    "\n",
    "- GlobalAveragePooling1D 계층은 시퀀스 차원을 평균화하여 각 예제에 대해 고정 길이 출력 벡터를 반환합니다. 이를 통해 모델은 가능한 가장 간단한 방법으로 가변 길이 입력을 처리 할 수 ​​있습니다.\n",
    "\n",
    "- 고정 길이 출력 벡터는 16 개의 은닉 유닛이있는 완전 연결 ( Dense ) 계층을 통해 파이프됩니다.\n",
    "\n",
    "- 마지막 레이어는 단일 출력 노드와 조밀하게 연결됩니다.\n",
    "\n",
    "주의 : 이 모델은 마스킹을 사용하지 않으므로 제로 패딩이 입력의 일부로 사용되므로 패딩 길이가 출력에 영향을 미칠 수 있습니다. 이 문제를 해결하려면 마스킹 및 패딩 가이드를 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:35.510031Z",
     "iopub.status.busy": "2021-01-13T04:09:35.509235Z",
     "iopub.status.idle": "2021-01-13T04:09:35.514781Z",
     "shell.execute_reply": "2021-01-13T04:09:35.514166Z"
    },
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dense(16, activation='relu'),\n",
    "  Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "### 모델 컴파일 및 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpX9etB6IOQd"
   },
   "source": [
    "TensorBoard 를 사용하여 손실 및 정확도를 포함한 메트릭을 시각화합니다. tf.keras.callbacks.TensorBoard 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:35.520770Z",
     "iopub.status.busy": "2021-01-13T04:09:35.520091Z",
     "iopub.status.idle": "2021-01-13T04:09:35.522572Z",
     "shell.execute_reply": "2021-01-13T04:09:35.521972Z"
    },
    "id": "W4Hg3IHFt4Px"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": [
    "Adam 최적화 프로그램과 BinaryCrossentropy 손실을 사용하여 모델을 컴파일하고 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:35.537946Z",
     "iopub.status.busy": "2021-01-13T04:09:35.537179Z",
     "iopub.status.idle": "2021-01-13T04:09:35.547747Z",
     "shell.execute_reply": "2021-01-13T04:09:35.548233Z"
    },
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:09:35.553172Z",
     "iopub.status.busy": "2021-01-13T04:09:35.552452Z",
     "iopub.status.idle": "2021-01-13T04:10:00.939480Z",
     "shell.execute_reply": "2021-01-13T04:10:00.938836Z"
    },
    "id": "5mQehiQyv8rP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 4s 195ms/step - loss: 0.6924 - accuracy: 0.5028 - val_loss: 0.6906 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 2s 122ms/step - loss: 0.6885 - accuracy: 0.5028 - val_loss: 0.6856 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 2s 110ms/step - loss: 0.6814 - accuracy: 0.5028 - val_loss: 0.6761 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 2s 105ms/step - loss: 0.6691 - accuracy: 0.5028 - val_loss: 0.6620 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 2s 109ms/step - loss: 0.6517 - accuracy: 0.5028 - val_loss: 0.6431 - val_accuracy: 0.4886\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 2s 105ms/step - loss: 0.6286 - accuracy: 0.5128 - val_loss: 0.6193 - val_accuracy: 0.5302\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 2s 102ms/step - loss: 0.6004 - accuracy: 0.5868 - val_loss: 0.5920 - val_accuracy: 0.6028\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 2s 112ms/step - loss: 0.5686 - accuracy: 0.6651 - val_loss: 0.5629 - val_accuracy: 0.6604\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 2s 111ms/step - loss: 0.5351 - accuracy: 0.7188 - val_loss: 0.5341 - val_accuracy: 0.7014\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.5021 - accuracy: 0.7568 - val_loss: 0.5073 - val_accuracy: 0.7294\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 0.4710 - accuracy: 0.7825 - val_loss: 0.4836 - val_accuracy: 0.7492\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 2s 110ms/step - loss: 0.4429 - accuracy: 0.8019 - val_loss: 0.4632 - val_accuracy: 0.7620\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 0.4179 - accuracy: 0.8153 - val_loss: 0.4461 - val_accuracy: 0.7736\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 113ms/step - loss: 0.3957 - accuracy: 0.8288 - val_loss: 0.4319 - val_accuracy: 0.7810\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 2s 115ms/step - loss: 0.3758 - accuracy: 0.8390 - val_loss: 0.4199 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13da5d7a800>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": [
    "이 접근 방식을 사용하면 모델이 약 84 %의 검증 정확도에 도달합니다 (학습 정확도가 더 높기 때문에 모델이 과적 합된다는 점에 유의하십시오).\n",
    "\n",
    "참고 : 임베딩 레이어를 훈련하기 전에 가중치가 무작위로 초기화 된 방식에 따라 결과가 약간 다를 수 있습니다.\n",
    "\n",
    "모델 요약을 살펴보고 모델의 각 계층에 대해 자세히 알아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:10:00.945328Z",
     "iopub.status.busy": "2021-01-13T04:10:00.944605Z",
     "iopub.status.idle": "2021-01-13T04:10:00.949159Z",
     "shell.execute_reply": "2021-01-13T04:10:00.948611Z"
    },
    "id": "mDCgjWyq_0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 100)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiQbOJZ2WBFY"
   },
   "source": [
    "TensorBoard에서 모델 측정 항목을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_Uanp2YH8RzU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c3b0e82d99ef1868\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c3b0e82d99ef1868\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvURkGVpXDOy"
   },
   "source": [
    "![embeddings_classifier_accuracy.png](https://www.tensorflow.org/tutorials/text/images/embeddings_classifier_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": [
    "## 훈련 된 단어 임베딩을 검색하여 디스크에 저장\n",
    "\n",
    "다음으로 훈련 중에 학습 한 단어 임베딩을 검색합니다. 임베딩은 모델에있는 임베딩 레이어의 가중치입니다. 가중치 행렬은 형태 (vocab_size, embedding_dimension) 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "get_layer() 및 get_weights() 사용하여 모델에서 가중치를 얻습니다. get_vocabulary() 함수는 한 줄에 하나의 토큰으로 메타 데이터 파일을 빌드하기위한 어휘를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:10:00.954426Z",
     "iopub.status.busy": "2021-01-13T04:10:00.953719Z",
     "iopub.status.idle": "2021-01-13T04:10:00.969888Z",
     "shell.execute_reply": "2021-01-13T04:10:00.969296Z"
    },
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape = (10000, 16)\n",
      "weights = array([[-0.02457644, -0.04285206,  0.04310771, ...,  0.04748229,\n",
      "         0.05564587, -0.07776756],\n",
      "       [-0.10476382, -0.01762174,  0.04294935, ..., -0.07001776,\n",
      "         0.06002777, -0.12552263],\n",
      "       [-0.02971018, -0.0560519 ,  0.1573273 , ...,  0.02802005,\n",
      "         0.16388883, -0.2399383 ],\n",
      "       ...,\n",
      "       [-0.23004243,  0.17565645, -0.16232032, ..., -0.1503096 ,\n",
      "        -0.20936635,  0.20165841],\n",
      "       [ 0.18075262, -0.19324261,  0.15926781, ...,  0.12508431,\n",
      "         0.14169998, -0.16763668],\n",
      "       [-0.02230727,  0.00898734,  0.00481418, ...,  0.00326165,\n",
      "        -0.00897241, -0.03934529]], dtype=float32)\n",
      "type(vocab) = <class 'list'>\n",
      "len(vocab) = 10000\n",
      "vocab[:10] = ['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(f'{weights.shape = }')\n",
    "print(f'{weights = }')\n",
    "print(f'{type(vocab) = }')\n",
    "print(f'{len(vocab) = }')\n",
    "print(f'{vocab[:10] = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "디스크에 가중치를 씁니다. Embedding Projector 를 사용하려면 벡터 파일 (임베딩 포함)과 메타 데이터 파일 (단어 포함)의 두 파일을 탭으로 구분 된 형식으로 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-13T04:10:00.975706Z",
     "iopub.status.busy": "2021-01-13T04:10:00.974988Z",
     "iopub.status.idle": "2021-01-13T04:10:01.112373Z",
     "shell.execute_reply": "2021-01-13T04:10:01.112873Z"
    },
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": [
    "out_v = -1\n",
    "out_m = -1\n",
    "\n",
    "try:\n",
    "  out_v = open('vectors.tsv', 'w', encoding='utf-8')\n",
    "  out_m = open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "  for index, word in enumerate(vocab):\n",
    "    if  index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "except Exception as e:\n",
    "  print(e, file=sys.stderr)\n",
    "finally:\n",
    "  if out_v != -1:\n",
    "    out_v.close()\n",
    "  if out_m != -1:\n",
    "    out_m.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "eb1bacf077ed9b7db89eabb0d9092da33d9e6a64c9f4637d6488095aa0954d62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
